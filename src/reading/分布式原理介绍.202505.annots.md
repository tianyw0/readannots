# 分布式原理介绍.202505

1. 本文将讨论的重点集中在分布式层面的协议和算法设计

2. 首先给出在本文中研究的分布式系统在分布式层面的基本问题模型

3. 一个节点往往是一个操作系统上的进程。

4. 节点与节点之间是完全独立、相互隔离的，节点之间传递信息的唯一方式是通过不可靠的网络进行通信。

5. 。存储、读取数据的节点称为有状态的节点，反之称为无状态的节点

6. 分布式系统核心问题之一就是处理各种异常(failure)情况

7. 消息乱序是指节点发送的网络消息有一定的概率不是按照发送时的顺序依次到达目的节点

8. 。这就要求设计分布式协议时，考虑使用序列号等机制处理网络消息的乱序问题，使得无效的、过期的网络消息不影响系统的正确性。 

9. 网络上传输的数据有可能发生比特错误，从而造成数据错误。通常使用一定的校验码机制可以较为简单的检查出网络数据的错误，从而丢弃错误的数据。 

10. TCP 协议为应用层提供了可靠的、面向连接的传输服务。TCP 协议是最优秀的传输层协议之一，其设计初衷就是在不可靠的网络之上建立可靠的传输服务。TCP 协议通过为传输的每一个字节设置顺序递增的序列号，由接收方在收到数据后按序列号重组数据并发送确认信息，当发现数据包丢失时，TCP 协议重传丢失的数据包，从而TCP 协议解决了网络数据包丢失的问题和数据包乱序问题。TCP 协议为每个TCP 数据段（以太网上通常最大为1460 字节）使用32 位的校验和从而检查数据错误问题。TCP 协议通过设置接收和发送窗口的机制极大的提高了传输性能，解决了网络传输的时延与吞吐问题。TCP 协议最为复杂而巧妙的是其几十年来不断改进的拥塞控制算法，使得TCP 可以动态感知底层链路的带宽加以合理使用并与其他TCP 链接分享带宽（TCP friendly）。

11. ，在设计分布系统的网络协议时即使使用TCP 协议，也依旧要考虑网络异常，不能简单的认为使用TCP 协议后通信就是可靠的。另一方面，如果完全放弃使用TCP 协议，使用UDP协议加自定义的传输控制机制，则会使得系统设计复杂。尤其是要设计、实现一个像TCP 那样优秀的拥塞控制机制是非常困难的。 

12. RPC 执行的结果有三种状态：“成功”、“失败”、“超时（未知）”，称之为分布式系统的三态。 

13. 副本的概念副本（replica/copy）指在分布式系统中为数据或服务提供的冗余。对于数据副本指在不同的节点上持久化同一份数据，当出现某一个节点的存储的数据丢失时，可以从副本上读到数据。数据副本是分布式系统解决数据丢失异常的唯一手段。另一类副本是服务副本，指数个节点提供某种相同的服务，这种服务一般并不依赖于节点的本地存储，其所需数据一般来自其他节点。 

14. GFS 系统的同一个chunk 的数个副本就是典型的数据副本，而Map Reduce 系统的Job Worker 则是典型的服务副本

15. 副本协议是贯穿整个分布式系统的理论核心，在后续章节中，本文将讨论在工程中广泛使用的各种副本协议

16. 分布式系统通过副本控制协议，使得从系统外部读取系统内部各个副本的数据在一定的约束条件下相同，称之为副本一致性(consistency)。副

17. 依据一致性的强弱即约束条件的不同苛刻程度，副本一致性分为若干变种或者级别

18. 强一致性(strong consistency)：任何时刻任何用户或节点都可以读到最近一次成功更新的副本数据。强一致性是程度最高的一致性要求，也是实践中最难以实现的一致性

19. 单调一致性(monotonic consistency)：任何时刻，任何用户一旦读到某个数据在某次更新后的值，这个用户不会再读到比这个值更旧的值。单调一致性是弱于强一致性却非常实用的一种一致性级别。因为通常来说，用户只关心从己方视角观察到的一致性，而不会关注其他用户的一致性情况

20. 会话一致性(session consistency)：任何用户在某一次会话内一旦读到某个数据在某次更新后的值，这个用户在这次会话过程中不会再读到比这个值更旧的值。

21. 最终一致性(eventual consistency)：最终一致性要求一旦更新成功，各个副本上的数据最终将达到完全一致的状态，但达到完全一致状态所需要的时间不能保障。对于最终一致性系统而言，一个用户只要始终读取某一个副本的数据，则可以实现类似单调一致性的效果，但一旦用户更换读取的副本，则无法保障任何一致性。 弱一致性(week consistency)：一旦某个更新成功，用户无法在一个确定时间内读到这次更新的值，且即使在某个副本上读到了新的值，也不能保证在其他副本上可以读到新的值。弱一致性系统一般很难在实际中使用，使用弱一致性系统需要应用方做更多的工作从而使得系统可用

22. 评价分布式系统有一些常用的指标。依据设计需求的不同，分布式系统对于这些指标也有不同的要求。 

23. 常见的性能指标有：系统的吞吐能力，指系统在某一时间可以处理的数据总量，通常可以用系统每秒处理的总的数据量来衡量；系统的响应延迟，指系统完成某一功能需要使用的时间；系统的并发能力，指系统可以同时完成某一功能的能力，通常也用QPS(query per second)来衡量。上述三个性能指标往往会相互制约，追求高吞吐的系统，往往很难做到低延迟；系统平均响应时间较长时，也很难提高QPS。 

24. 系统的可用性(availability)指系统在面对各种异常时可以正确提供服务的能力。系

25. 系统的可扩展性(scalability)指分布式系统通过扩展集群机器规模提高系统性能（吞吐、延迟、并发）、存储容量、计算能力的特性。可扩展性是分布式系统的特有性质。分布式系统的设计初衷就是利用集群多机的能力处理单机无法解决的问题。

26. 当任务的需求随着具体业务不断提高时，除了升级系统的性能，另一个做法就是通过增加机器的方式扩展系统的规模。好的分布式系统总在追求“线性扩展性”，也就是使得系统的某一指标可以随着集群中的机器数量线性增长。 

27. 分布式系统为了提高可用性，总是不可避免的使用副本的机制，从而引发副本一致性的问题。根据具体的业务需求的不同，分布式系统总是提供某种一致性模型，并基于此模型提供具体的服务。

28. 如何拆解分布式系统的输入数据成为分布式系统的基本问题，本文称这样的数据拆解为数据分布方式，在本节中介绍几种常见的数据分布方式

29. 哈希方式是最常见的数据分布方式，其方法是按照数据的某一特征计算哈希值，并将哈希值与机器中的机器建立映射关系，从而将不同哈希值的数据分布到不同的机器上。所谓数据特征可以是key-value 系统中的key，也可以是其他与应用业务逻辑相关的值。

30. 哈希分布数据的缺点同样明显，突出表现为可扩展性不高，一旦集群规模需要扩展，则几乎所有的数据需要被迁移并重新分布

31. 工程中，扩展哈希分布数据的系统时，往往使得集群规模成倍扩展，按照数据重新计算哈希，这样原本一台机器上的数据只需迁移一半到另一台对应的机器上即可完成扩展

32. 哈希方式扩展性差的问题，一种思路是不再简单的将哈希值与机器做除法取模映射，而是将对应关系作为元数据由专门的元数据服务器管理。访问数据时，首先计算哈希值并查询元数据服务器，获得该哈希值对应的机器。同时，哈希值取模个数往往大于机器个数，这样同一台机器上需要负责多个哈希取模的余数。在集群扩容时，将部分余数分配到新加入的机器并迁移对应的数据到新机器上，从而使得扩容不再依赖于机器数量的成本增长

33. 这种做法和2.1.2 中按数据范围分布数据、2.1.3 按数据量分布数据的有一个共同点，都需要以较复杂的机制维护大量的元数据

34. 哈希分布数据的另一个缺点是，一旦某数据特征值的数据严重不均，容易出现“数据倾斜”（data skew）问题

35. 在这种情况下只能重新选择需要哈希的数据特征，例如选择用户id 与另一个数据维度的组合作为哈希函数的输入，如这样做，则需要完全重新分布数据，在工程实践中可操作性不高。另一种极端的思路是，使用数据的全部而不是某些维度的特征计算哈希，这样数据将被完全打散在集群中。然而实践中有时并不这样做，这是因为这样做使得每个数据之间的关联性完全消失，

36. 如果系统处理的每条数据之间没有任何逻辑上的联系，例如一个给定关键词的查询系统，每个关键词之间并没有逻辑上的联系，则可以使用全部数据做哈希的方式解决数据倾斜问题。 

37. 按数据范围分布是另一个常见的数据分布式，将数据按特征值的值域范围划分为不同的区间，使得集群中每台（组）服务器处理不同区间的数据

38. 与哈希分布数据的方式只需要记录哈希函数及分桶个数（机器数）不同，按数据范围分布数据需要记录所有的数据分布情况。一般的，往往需要使用专门的服务器在内存中维护数据分布信息，称这种数据的分布信息为一种元信息。甚至对于大规模的集群，由于元信息的规模非常庞大，单台计算机无法独立维护，需要使用多台机器作为元信息服务器

39. 哈希分布数据的方式使得系统中的数据类似一个哈希表。按范围分数据的方式则使得从全局看数据类似一个B 树

40. 数据量分布数据与具体的数据特征无关，而是将数据视为一个顺序增长的文件，并将这个文件按照某一较为固定的大小划分为若干数据块（chunk），不同的数据块分布到不同的服务器上。与按数据范围分布数据的方式类似的是，按数据量分布数据也需要记录数据块的具体分布情况，并将该分布信息作为元数据使用元数据服务器管理。 

41. 按数据量划分数据的缺点是需要管理较为复杂的元信息，与按范围分布数据的方式类似，当集群规模较大时，元信息的数据量也变得很大，高效的管理元信息成为新的课题。 

42. 一致性哈希（consistent hashing）是另一个种在工程中使用较为广泛的数据分布方式

43. 用一个哈希函数计算数据或数据特征的哈希值，令该哈希函数的输出值域为一个封闭的环，即哈希函数输出的最大值是最小值的前序。将节点随机分布到这个环上，每个节点负责处理从自己开始顺时针至下一个节点的全部哈希值域上的数据。 

44. 一致性哈希的优点在于可以任意动态添加、删除节点，每次添加、删除一个节点仅影响一致性哈希环上相邻的节点

45. 最基本的一致性哈希算法有很明显的缺点，随机分布节点的方式使得很难均匀的分布哈希值域，尤其在动态增加节点后，即使原先的分布均匀也很难保证继续均匀，由此带来的另一个较为严重的缺点是，当一个节点异常时，该节点的压力全部转移到相邻的一个节点，当加入一个新节点时只能为一个相邻节点分摊压力

46. 一种常见的改进算法是引入虚节点（virtual node）的概念，系统初始时就创建许多虚节点，虚节点的个数一般远大于未来集群中机器的个数，将虚节点均匀分布到一致性哈希值域环上

47. 为每个节点分配若干虚节点。操作数据时，首先通过数据的哈希值在环上找到对应的虚节点，进而查找元数据找到对应的真实节点

48. 使用虚节点改进有多个优点。首先，一旦某个节点不可用，该节点将使得多个虚节点不可用，从而使得多个相邻的真实节点负载失效节点的压里。同理，一旦加入一个新节点，可以分配多个虚节点，从而使得新节点可以负载多个原有节点的压力

49. 上述几节讨论了几种常见的数据分布方式，这些讨论中没有考虑数据副本的问题。分布式系统容错、提高可用性的基本手段就是使用副本。对于数据副本的分布方式主要影响系统的可扩展性

50. 一种基本的数据副本策略是以机器为单位，若干机器互为副本，副本机器之间的数据完全相同。这种策略适用于上述各种数据分布方式。其优点是非常简单，其缺点是恢复数据的效率不高、可扩展性也不高。

51. 更合适的做法不是以机器作为副本单位，而是将数据拆为较合理的数据段，以数据段为单位作为副本。实践中，常常使得每个数据段的大小尽量相等且控制在一定的大小以内。数据段有很多不同的称谓，segment，fragment，chunk，partition 等等。数据段的选择与数据分布方式直接相关

52. 一旦将数据分为数据段，则可以以数据段为单位管理副本，从而副本与机器不再硬相关，每台机器都可以负责一定数据段的副本

53. 一旦副本分布与机器无关，数据丢失后的恢复效率将非常高。这是因为，一旦某台机器的数据丢失，其上数据段的副本将分布在整个集群的所有机器中，而不是仅在几个副本机器中，从而可以从整个集群同时拷贝恢复数据，而集群中每台数据源机器都可以以非常低的资源做拷贝

54. 副本分布与机器无关也利于集群容错。如果出现机器宕机，由于宕机机器上的副本分散于整个集群，其压力也自然分散到整个集群。最后，副本分布与机器无关也利于集群扩展。理论上，设集群规模为N 台机器，当加入一台新的机器时，只需从各台机器上迁移1/N – 1/N+1 比例的数据段到新机器即实现了新的负载均衡。由于是从集群中各机器迁移数据，与数据恢复同理，效率也较高

55. 完全按照数据段建立副本会引起需要管理的元数据的开销增大，副本维护的难度也相应增大。一种折中的做法是将某些数据段组成一个数据段分组，按数据段分组为粒度进行副本管理。这样做可以将副本粒度控制在一个较为合适的范围内

56. 几乎所有的分布式系统都会涉及到数据分布问题。这里列举了几个常见的分布式系统的数据分布方式如下。 GFS[9] & HDFS按数据量分布 Map reduce[10] 按GFS 的数据分布做本地化Big Table[11] & HBase 按数据范围分布 PNUTS[14] 哈希方式/按数据范围分布（可选） Dynamo[16] & Cassandra[17]一致性哈希 Mola & Armor *[18]哈希方式 Big Pipe *[18]哈希方式 Doris *[18]哈希方式与按数据量分布组合 

57. 本节讨论基本的副本控制协议，着重分析两大类典型的副本控制协议

58. 副本控制协议指按特定的协议流程控制副本数据的读写行为，使得副本满足一定的可用性和一致性要求的分布式协议

59. 副本控制协议要具有一定的对抗异常状态的容错能力，从而使得系统具有一定的可用性，同时副本控制协议要能提供一定一致性级别

60. 本文将副本控制协议分为两大类：
“中心化(centralized)副本控制协议”和“去中心化(decentralized)副本控制协议”。 

61. 中心化副本控制协议的基本思路是由一个中心节点协调副本数据的更新、维护副本之间的一致性

62. 中心化副本控制协议的优点是协议相对较为简单，所有的副本相关的控制交由中心节点完成

63. 并发控制将由中心节点完成，从而使得一个分布式并发控制问题，简化为一个单机并发控制问题

64. 所谓并发控制，即多个节点同时需要修改副本数据时，需要解决“写写”、“读写”等并发冲突。单机系统上常用加锁等方式进行并发控制

65. 本文着重介绍一种非常常用的primary-secondary（也称primary-backup）的中心化副本控制协议。在primary-secondary 类型的协议中，副本被分为两大类，其中有且仅有一个副本作为primary 副本，除primary 以外的副本都作为secondary 副本。维护primary 副本的节点作为中心节点，中心节点负责维护数据的更新、并发控制、协调副本的一致性

66. Primary-secondary 类型的协议一般要解决四大类问题：数据更新流程、数据读取方式、Primary副本的确定和切换、数据同步（reconcile）。

67. 使用primary-secondary 比较困难的是实现强一致性。 这里简单讨论primary-secondary 实现强一致性的几种思路

68. 第一、由于数据的更新流程都是由primary 控制的，primary 副本上的数据一定是最新的，所以如果始终只读primary 副本的数据，可以实现强一致性

69. 每台机器上都有一些数据的primary 副本，也有另一些数据段的secondary 副本。从而某台服务器实际都提供读写服务

70. 第二、由primary 控制节点secondary 节点的可用性

71. 这种方式依赖于一个中心元数据管理系统，用于记录哪些副本可用，哪些副本不可用。某种意义上，该方式通过降低系统的可用性来提高系统的一致性

72. 在primary-secondary 类型的协议中，另一个核心的问题是如何确定primary 副本，尤其是在原primary 副本所在机器出现宕机等异常时，需要有某种机制切换primary 副本，使得某个secondary副本成为新的primary 副本。通常的，在primary-secondary 类型的分布式系统中，哪个副本是primary 这一信息都属于元信息，由专门的元数据服务器维护。执行更新操作时，首先查询元数据服务器获取副本的primary 信息，从而进一步执行数据更新流程

73. 切换副本的难点在于两个方面，首先，如何确定节点的状态以发现原primary 节点异常是一个较为复杂的问题。在2.3 中，详细介绍一种基于Lease 机制确定节点状态的方法

74. 然而在原primary 已经发送宕机等异常时，如何确定一个secondary 副本使得该副本上的数据与原primary 一致又成为新的问题

75. 由于分布式系统中可靠的发现节点异常是需要一定的探测时间的

76. primary-backup 类副本协议的最大缺点就是由于primary 切换带来的一定的停服务时间

77. Primary-secondary 型协议一般都会遇到secondary 副本与primary 不一致的问题。此时，不一致的secondary 副本需要与primary 进行同步（reconcile）。

78. 通常不一致的形式有三种：一、由于网络分化等异常，secondary 上的数据落后于primary 上的数据。二、在某些协议下，secondary 上的数据有可能是脏数据，需要被丢弃。所谓脏数据是由于primary 副本没有进行某一更新操作，而secondary 副本上反而进行的多余的修改操作，从而造成secondary 副本数据错误。三、secondary 

79. 去中心化副本控制是另一类较为复杂的副本控制协议

80. 去中心化副本控制协议没有中心节点，协议中所有的节点都是完全对等的，节点之间通过平等协商达到一致。从而去中心化协议没有因为中心化节点异常而带来的停服务等问题

81. 然而，没有什么事情是完美的，去中心化协议的最大的缺点是协议过程通常比较复杂。尤其当去中心化协议需要实现强一致性时，协议流程变得复杂且不容易理解。由于流程的复杂，去中心化协议的效率或者性能一般也较中心化协议低

82. 与中心化副本控制协议具有某些共性不同，各类去中心化副本控制协议则各有各的巧妙

83. Paxos 是唯一在工程中得到应用的强一致性去中心化副本控制协议

84. 工程中大量的副本控制协议都是primary-secondary 型协议。从下面这些具体的分布式系统中不难看出，Primary-secondary 型副本控制虽然简单，但使用却极其广泛

85. Lease 机制是最重要的分布式协议，广泛应用于各种实际的分布式系统中

86. lease 机制最重要的应用：判定节点状态

87. ：Lease 是由颁发者授予的在某一有效期内的承诺。颁发者一旦发出lease，则无论接受方是否收到，也无论后续接收方处于何种状态，只要lease 不过期，颁发者一定严守承诺；另一方面，接收方在lease 的有效期内可以使用颁发者的承诺，但一旦lease 过期，接收方一定不能继续使用颁发者的承诺

88. Lease 机制具有很高的容错能力

89. 。首先，通过引入有效期，Lease 机制能否非常好的容错网络异常。Lease 颁发过程只依赖于网络可以单向通信，即使接收方无法向颁发者发送消息，也不影响lease的颁发。由于lease 的有效期是一个确定的时间点，lease 的语义与发送lease 的具体时间无关，所以同一个lease 可以被颁发者不断重复向接受方发送。即使颁发者偶尔发送lease 失败，颁发者也可以简单的通过重发的办法解决。一旦lease 被接收方成功接受，后续lease 机制不再依赖于网络通信，即使网络完全中断lease 机制也不受影响

90. 。再者，Lease 机制能较好的容错节点宕机

91. 最后，lease 机制不依赖于存储

92. Lease 机制依赖于有效期，这就要求颁发者和接收者的时钟是同步的

93. 对于这种时钟不同步，实践中的通常做法是将颁发者的有效期设置得比接收者的略大，只需大过时钟误差就可以避免对lease 的有效性的影响

94. 在分布式系统中确定一个节点是否处于正常工作状态是一个困难的问题。由于可能存在网络分化，节点的状态是无法通过网络通信来确定的。下面举一个较为具体的例子来讨论这个问题

95. 例2.3.1：在一个primary-secondary 架构的系统中，有三个节点A、B、C 互为副本，其中有一个节点为primary，且同一时刻只能有一个primary 节点。另有一个节点Q 负责判断节点A、B、C的状态，一旦Q 发现primary 异常，节点Q 将选择另一个节点作为primary。假设最开始时节点A为primary，B、C 为secondary。节点Q 需要判断节点A、B、C 的状态是否正常

96. 首先需要说明的是基于“心跳”(Heartbeat)的方法无法很好的解决这个问题

97. 由于节点Q 的通知消息到达节点A、B、C 的顺序无法确定，假如先到达B，则在这一时刻，系统中同时存在两个工作中的primary，一个是A、另一个是B。假如此时A、B 都接收外部请求并与C 同步数据，会产生严重的数据错误。上述即所谓“双主”问题，虽然看似这种问题出现的概率非常低，但在工程实践中，笔者不止一次见到过这样的情况发生

98. 上述问题的出现的原因在于虽然节点Q 认为节点A 异常，但节点A 自己不认为自己异常，依旧作为primary 工作。其问题的本质是由于网络分化造成的系统对于“节点状态”认知的不一致

99. 上面的例子中的分布式协议依赖于对节点状态认知的全局一致性，即一旦节点Q 认为某个节点A 异常，则节点A 也必须认为自己异常，从而节点A 停止作为primary，避免“双主”问题的出现。解决这种问题有两种思路，第一、设计的分布式协议可以容忍“双主”错误，即不依赖于对节点状态的全局一致性认识，或者全局一致性状态是全体协商后的结果；第二、利用lease 机制

100. 由中心节点向其他节点发送lease，若某个节点持有有效的lease，则认为该节点正常可以提供服务。用于例2.3.1 中，节点A、B、C 依然周期性的发送heart beat 报告自身状态，节点Q 收到heart beat后发送一个lease，表示节点Q 确认了节点A、B、C 的状态，并允许节点在lease 有效期内正常工作。节点Q 可以给primary 节点一个特殊的lease，表示节点可以作为primary 工作。一旦节点Q 希望切换新的primary，则只需等前一个primary 的lease 过期，则就可以安全的颁发新的lease 给新的primary 节点，而不会出现“双主”问题

101. 。工程中，常选择的lease 时长是10 秒级别，这是一个经过验证的经验值，实践中可以作为参考并综合选择合适的时长

102. 。直接实现lease机制的确会对增加系统设计的复杂度。然而，由于有类似Zookeeper 这样的开源的高可用系统，在工程中完全可以间接使用Lease。借助zookeeper，我们可以简单的实现高效的、无单点选主、状态监控、分布式锁、分布式消息队列等功能，而实际上，这些功能的实现都是依赖于背后zookeeper与client 之间的Lease 的

103. Quorum 机制是一种简单有效的副本管理机制

104. 本节首先讨论一种最简单的副本控制规则write-all-read-one，在此基础上，放松约束，讨论quorum 机制

105. Write-all-read-one（简称WARO）是一种最简单的副本控制规则，顾名思义即在更新时写所有的副本，只有在所有的副本上更新成功，才认为更新成功，从而保证所有的副本一致，这样在读取数据时可以读任一副本上的数据

106. 在Quorum 机制下，当某次更新操作wi一旦在所有N 个副本中的W 个副本上都成功，则就称该更新操作为“成功提交的更新操作”，称对应的数据为“成功提交的数据”。

107. 。令R>N-W，由于更新操作wi仅在W 个副本上成功，所以在读取数据时，最多需要读取R 个副本则一定能读到wi 更新后的数据vi 。如果某次更新wi 在W 个副本上成功，由于W+R>N，任意R 个副本组成的集合一定与成功的W 个副本组成的集合有交集，所以读取R 个副本一定能读到wi 更新后的数据vi

108. 在上述定义中，令W=N，R=1，就得到WARO，即WARO 是Quorum 机制的一种特例

109. 与分析WARO 相似，分析Quorum 机制的可用性。限制Quorum 参数为W+R=N+1。由于更新操作需要在W 个副本上都成功，更新操作才能成功，所以一旦N-W+1 个副本异常，更新操作始终无法在W 个副本上成功，更新服务不可用。另一方面，一旦N-R+1 个副本异常，则无法保证一定可以读到与W 个副本有交集的副本集合，则读服务的一致性下降

110. ：仅仅依赖quorum 机制是无法保证强一致性的。因为仅有quorum 机制时无法确定最新已成功提交的版本号，除非将最新已提交的版本号作为元数据由特定的元数据服务器或元数据集群管理，否则很难确定最新成功提交的版本号

111. Quorum 机制的三个系统参数N、W、R 控制了系统的可用性，也是系统对用户的服务承诺：数据最多有N 个副本，但数据更新成功W 个副本即返回用户成功。对于一致性要求较高的Quorum 系统，系统还应该承诺任何时候不读取未成功提交的数据，即读取到的数据都是曾经在W 个副本上成功的数据

112. 单纯使用Quorum 机制时，若要确定最新的成功提交的版本，最多需要读取R+（W-R-1）=N 个副本，当出现任一副本异常时，读最新的成功提交的版本这一功能都有可能不可用。实际工程中，应该尽量通过其他技术手段，回避通过Quorum 机制读取最新的成功提交的版本。例如，当quorum 机制与primary-secondary 控制协议结合使用时，可以通过读取primary 的方式读取到最新的已提交的数据

113. 本节介绍一种介于quorum 机制选择primary 的技术

